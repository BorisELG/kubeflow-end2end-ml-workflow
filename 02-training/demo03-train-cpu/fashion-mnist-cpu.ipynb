{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Image Classification - Single Node - CPU training\n",
    "\n",
    "**Code tested on:**\n",
    "\n",
    "- Tensorflow==2.1.0\n",
    "- Tensorflow-datasets==2.1.0\n",
    "- google-cloud-storage==1.26.0\n",
    "- pandas==1.0.3\n",
    "\n",
    "**Key activities**\n",
    "\n",
    "- Extract and process Fashion-MNIST data\n",
    "- Build Tensorflow keras model \n",
    "- Training on CPU and log the metrics ( using custom Keras logger )\n",
    "- Evaluate model \n",
    "- Use Tensorboard to visualize the training process. Use kubernetes port-forward to open the tensorboard running on K8S cluster\n",
    "- Model save and export\n",
    "- Model upload to GCS\n",
    "- Metadata tracking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tfds.disable_progress_bar()\n",
    "import logging\n",
    "from datetime import datetime\n",
    "logger = tf.get_logger()\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "    level=logging.INFO)\n",
    "print('Tensorflow-version: {0}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the logs\n",
    "!rm -rf logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction & processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "def prepare_data(batch_size=64, shuffle_size=1000):\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "        return image, label\n",
    "    \n",
    "    # Split the training set into 80% and 20% for training and validation\n",
    "    train_validation_split = tfds.Split.TRAIN.subsplit([8, 2])\n",
    "    ((train_data, validation_data), test_data),info = tfds.load(name=\"fashion_mnist:1.0.0\", \n",
    "                                                         split=(train_validation_split, tfds.Split.TEST),\n",
    "                                                         as_supervised=True, with_info=True)\n",
    "\n",
    "    \n",
    "    print(\"Training data count : \", int(info.splits['train'].num_examples * 0.8))\n",
    "    print(\"Validation data count : \", int(info.splits['train'].num_examples * 0.2))\n",
    "    print(\"Test data count : \", int(info.splits['test'].num_examples))\n",
    "\n",
    "\n",
    "    # create dataset to be used for training process\n",
    "    train_dataset = train_data.map(scale).shuffle(shuffle_size).batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_dataset = validation_data.map(scale).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_data.map(scale).batch(batch_size)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate=0.001):\n",
    "    # define model architecture\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28, 28, 1), name='x'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "      ])\n",
    "    # compile model with loss, optimizer and accuracy \n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    # callbacks \n",
    "    # folder to store current training logs\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    class customLog(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            logging.info('epoch: {}'.format(epoch + 1))\n",
    "            logging.info('loss={}'.format(logs['loss']))\n",
    "            logging.info('accuracy={}'.format(logs['accuracy']))\n",
    "            logging.info('val_accuracy={}'.format(logs['val_accuracy']))\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(logdir),\n",
    "        customLog()\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction and processing\n",
    "# set variables\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = prepare_data(batch_size=BATCH_SIZE, shuffle_size=BUFFER_SIZE)\n",
    "\n",
    "# Build Model\n",
    "TF_LEARNING_RATE = 0.001\n",
    "model = build_model(learning_rate=TF_LEARNING_RATE)\n",
    "model.summary()\n",
    "\n",
    "# train model\n",
    "TF_EPOCHS=10\n",
    "TF_STEPS_PER_EPOCHS=3\n",
    "#TF_STEPS_PER_EPOCHS = int(np.ceil(48000 / float(BATCH_SIZE)))  # number of training samples / batch size\n",
    "\n",
    "model.fit(train_dataset, \n",
    "          epochs=TF_EPOCHS,\n",
    "          steps_per_epoch=3,\n",
    "          validation_data=val_dataset,\n",
    "          callbacks=get_callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "result = model.evaluate(test_dataset, steps=1)\n",
    "loss = result[0]\n",
    "accuracy = result[1]\n",
    "print(\"loss : {0} accuracy : {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard\n",
    "Note : If you want to use Tensorboard : use tensorboard command and run from the terminal ( from notebook home page -> new -> terminal )\n",
    "\n",
    "```\n",
    "tensorboard --logdir=/home/jovyan/logs/ --bind_all\n",
    "```\n",
    "if you are running inside a **container** you can use **port-mapping**. if you are running inside **kubernetes pod**, then use the pod **port-forward feature** on the port 6006 (default for tensorboard, change it as per the tensorboard command output ). When a notebook is created, a pod with name <NOTEBOOK_NAME>-0 is created in the users namespace. So you can use the port-forward to access tensorboard. \n",
    "\n",
    "```\n",
    "kubectl port-forward -n <NAMESPACE> <NOTEBOOK_NAME>-0 6006:6006\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model save and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save keras model \n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model from saved model\n",
    "model_loaded = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate summary\n",
    "model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "tf.saved_model.save(model_loaded, \"export/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir export/ --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save on GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import glob\n",
    "from google.cloud import storage\n",
    "import re\n",
    "import logging\n",
    "\n",
    "_GCS_PREFIX = \"gs://\"\n",
    "\n",
    "\n",
    "class Storage(object):\n",
    "    @staticmethod\n",
    "    def upload(uri: str, out_dir: str = None) -> str:\n",
    "        logging.info(\"Copying contents from %s to %s\", uri, out_dir)\n",
    "\n",
    "        if out_dir.startswith(_GCS_PREFIX):\n",
    "            Storage._upload_gcs(uri, out_dir)\n",
    "        else:\n",
    "            raise Exception(\"Cannot recognize storage type for \" + uri +\n",
    "                            \"\\n'%s' are the current available storage type.\" %\n",
    "                            (_GCS_PREFIX))\n",
    "\n",
    "        logging.info(\"Successfully copied %s to %s\", uri, out_dir)\n",
    "        return out_dir\n",
    "    \n",
    "    @staticmethod\n",
    "    def _upload_gcs(uri, out_dir: str):\n",
    "        try:\n",
    "            storage_client = storage.Client()\n",
    "        except exceptions.DefaultCredentialsError:\n",
    "            storage_client = storage.Client.create_anonymous_client()\n",
    "        \n",
    "        bucket_args = out_dir.replace(_GCS_PREFIX, \"\", 1).split(\"/\", 1)\n",
    "        bucket_name = bucket_args[0]\n",
    "        gcs_path = bucket_args[1] if len(bucket_args) > 1 else \"\"\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        Storage.upload_local_directory_to_gcs(uri,bucket, gcs_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def upload_local_directory_to_gcs(local_path, bucket, gcs_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            if not os.path.isfile(local_file):\n",
    "                Storage.upload_local_directory_to_gcs(local_file, bucket, gcs_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(gcs_path, local_file[1 + len(local_path):])\n",
    "                blob = bucket.blob(remote_path)\n",
    "                blob.upload_from_filename(local_file)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo ${GOOGLE_APPLICATION_CREDENTIALS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# get project ID \n",
    "PROJECT_ID=$(gcloud config get-value core/project)\n",
    "# create bucket \n",
    "BUCKET=${PROJECT_ID}-fashion-mnist\n",
    "# delete the bucket if exists before - CAUTION: THIS WILL REMOVE THE BUCKET\n",
    "gsutil rm -r gs://$BUCKET\n",
    "# create the bucket\n",
    "gsutil mb gs://$BUCKET/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id=!gcloud config get-value core/project\n",
    "bucket=\"gs://{0}-fashion-mnist/export/001\".format(project_id[0])\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Storage.upload(\"export\",bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --user kubeflow-metadata==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.metadata import metadata\n",
    "import pandas\n",
    "from datetime import datetime\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata store details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_STORE_HOST = \"metadata-grpc-service.kubeflow\" # default DNS of Kubeflow Metadata gRPC serivce.\n",
    "METADATA_STORE_PORT = 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = metadata.Workspace(\n",
    "    # Connect to metadata service in namespace kubeflow in k8s cluster.\n",
    "    store=metadata.Store(grpc_host=METADATA_STORE_HOST, grpc_port=METADATA_STORE_PORT),\n",
    "    name=\"workspace_abhi\",\n",
    "    description=\"workspace for fashion-mnist\",\n",
    "    labels={\"name\": \"user-1\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = metadata.Run(\n",
    "    workspace=workspace,\n",
    "    name=\"run-\" + datetime.utcnow().isoformat(\"T\") ,\n",
    "    description=\"deep learning based model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec = metadata.Execution(\n",
    "    name = \"execution\" + datetime.utcnow().isoformat(\"T\") ,\n",
    "    workspace=workspace,\n",
    "    run=run,\n",
    "    description=\"convolutional network with 32 filters\",\n",
    ")\n",
    "print(\"An execution was created with id %s\" % exec.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"model_version_\" + str(uuid4())\n",
    "model = exec.log_output(\n",
    "    metadata.Model(\n",
    "            name=\"Fashion-MNIST\",\n",
    "            description=\"model to recognize fashion items\",\n",
    "            owner=\"test@kubeflow.org\",\n",
    "            uri=\"gcs://my-bucket/fashion-mnist\",\n",
    "            model_type=\"convolutional neural network\",\n",
    "            training_framework={\n",
    "                \"name\": \"tensorflow\",\n",
    "                \"version\": \"v2.1.0\"\n",
    "            },\n",
    "            hyperparameters={\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"layers\": [32, 64, 10],\n",
    "                \"early_stop\": True\n",
    "            },\n",
    "            version=model_version,\n",
    "            labels={\"mylabel\": \"l1\"}))\n",
    "print(model)\n",
    "print(\"\\nModel id is {0.id} and version is {0.version}\".format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_set_version = \"data_set_version_\" + str(uuid4())\n",
    "data_set = exec.log_input(\n",
    "        metadata.DataSet(\n",
    "            description=\"fashion-mnist dataset\",\n",
    "            name=\"fashion-mnist-dump\",\n",
    "            owner=\"owner@my-company.org\",\n",
    "            uri=\"gcs://my-bucket/fashion-mnist\",\n",
    "            version=date_set_version,\n",
    "            query=\"SELECT * FROM fashion-mnist\"))\n",
    "print(\"Data set id is {0.id} with version '{0.version}'\".format(data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = exec.log_output(\n",
    "    metadata.Metrics(\n",
    "            name=\"Fashion-MNIST-evaluation\",\n",
    "            description=\"validating the Fashion-MNIST model\",\n",
    "            owner=\"someone@kubeflow.org\",\n",
    "            uri=\"gcs://my-bucket/fashion-mnist-eval.csv\",\n",
    "            data_set_id=str(data_set.id),\n",
    "            model_id=str(model.id),\n",
    "            metrics_type=metadata.Metrics.VALIDATION,\n",
    "            values={\"loss\": str(loss), \"accuracy\": str(accuracy)},\n",
    "            labels={\"mylabel\": \"l1\"}))\n",
    "print(\"Metrics id is %s\" % metrics.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame.from_dict(workspace.list(metadata.Model.ARTIFACT_TYPE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame.from_dict(workspace.list(metadata.Metrics.ARTIFACT_TYPE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
