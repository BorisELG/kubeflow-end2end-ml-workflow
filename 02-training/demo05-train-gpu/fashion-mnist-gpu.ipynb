{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST Image Classification - Multi-GPU training\n",
    "\n",
    "**Code tested on:**\n",
    "\n",
    "- Tensorflow==2.1.0\n",
    "- Tensorflow-datasets==2.1.0\n",
    "\n",
    "\n",
    "**Key activities**\n",
    "\n",
    "- Extract and process Fashion-MNIST data\n",
    "- Build Tensorflow keras model \n",
    "- Training on Multiple GPU using MirroredStrategy \n",
    "- Evaluate model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (3.11.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (1.18.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (0.18.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (1.13.0)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (2.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (19.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (4.43.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (2.22.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (1.11.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (0.3.1.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (0.9.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==2.1.0) (0.21.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets==2.1.0) (44.0.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./.local/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets==2.1.0) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets==2.1.0) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow-datasets==2.1.0) (2.6)\n",
      "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets==2.1.0) (1.51.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-datasets==2.1.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "\n",
    "restartkernel() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tfds.disable_progress_bar()\n",
    "import logging\n",
    "from datetime import datetime\n",
    "logger = tf.get_logger()\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "    level=logging.INFO)\n",
    "print('Tensorflow-version: {0}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the logs\n",
    "!rm -rf logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction & processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "def prepare_data(batch_size=64, shuffle_size=1000):\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "        return image, label\n",
    "    \n",
    "    # Split the training set into 80% and 20% for training and validation\n",
    "    train_validation_split = tfds.Split.TRAIN.subsplit([8, 2])\n",
    "    ((train_data, validation_data), test_data),info = tfds.load(name=\"fashion_mnist:1.0.0\", \n",
    "                                                         split=(train_validation_split, tfds.Split.TEST),\n",
    "                                                         as_supervised=True, with_info=True)\n",
    "\n",
    "    \n",
    "    print(\"Training data count : \", int(info.splits['train'].num_examples * 0.8))\n",
    "    print(\"Validation data count : \", int(info.splits['train'].num_examples * 0.2))\n",
    "    print(\"Test data count : \", int(info.splits['test'].num_examples))\n",
    "\n",
    "\n",
    "    # create dataset to be used for training process\n",
    "    train_dataset = train_data.map(scale).shuffle(shuffle_size).batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_dataset = validation_data.map(scale).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_data.map(scale).batch(batch_size)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate=0.001):\n",
    "    # define model architecture\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28, 28, 1), name='x'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "      ])\n",
    "    # compile model with loss, optimizer and accuracy \n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    # callbacks \n",
    "    # folder to store current training logs\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    class customLog(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            logging.info('epoch: {}'.format(epoch + 1))\n",
    "            logging.info('loss={}'.format(logs['loss']))\n",
    "            logging.info('accuracy={}'.format(logs['accuracy']))\n",
    "            logging.info('val_accuracy={}'.format(logs['val_accuracy']))\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(logdir),\n",
    "        customLog()\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list physical devices available\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using MirroredStrategy\n",
    "NUM_GPUS = 2\n",
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # Data extraction and processing\n",
    "    # set variables\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = prepare_data(batch_size=BATCH_SIZE, shuffle_size=BUFFER_SIZE)\n",
    "    \n",
    "    TF_LEARNING_RATE = 0.001\n",
    "    # build model\n",
    "    model = build_model(learning_rate=TF_LEARNING_RATE)\n",
    "    model.summary()\n",
    "    # train model\n",
    "    TF_EPOCHS=20\n",
    "    TF_STEPS_PER_EPOCHS = int(np.ceil(60000 / float(BATCH_SIZE)))  \n",
    "\n",
    "    model.fit(train_dataset, \n",
    "              epochs=TF_EPOCHS,\n",
    "              steps_per_epoch=3,\n",
    "              validation_data=val_dataset,\n",
    "              callbacks=get_callbacks())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Track GPU Usage** \n",
    "\n",
    "If you want to track the GPU usage then, open a terminal and use `nvidia-smi` command. To get refreshed value you can use the `watch -n <NUM_SECONDS>` command. \n",
    "\n",
    "`watch -n 1 nvidia-smi`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "result = model.evaluate(test_dataset, steps=1)\n",
    "loss = result[0]\n",
    "accuracy = result[1]\n",
    "print(\"loss : {0} accuracy : {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard\n",
    "Note : If you want to use Tensorboard : use tensorboard command \n",
    "\n",
    "```\n",
    "tensorboard --logdir=/home/jovyan/logs/ --bind_all\n",
    "```\n",
    "if you are running inside a **container** you can use **port-mapping**. if you are running inside **kubernetes pod**, then use the pod **port-forward feature** on the port 6006 (default for tensorboard, change it as per the tensorboard command output ). When a notebook is created, a pod with name <NOTEBOOK_NAME>-0 is created in the users namespace. So you can use the port-forward to access tensorboard. \n",
    "\n",
    "```\n",
    "kubectl port-forward -n <NAMESPACE> <NOTEBOOK_NAME>-0 6006:6006\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
