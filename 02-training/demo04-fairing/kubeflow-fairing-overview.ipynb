{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubeflow Fairing Overview\n",
    "\n",
    "Key Activities\n",
    "\n",
    "- Create model class \n",
    "- Train locally\n",
    "- Train on Kubernetes cluster\n",
    "- Train on Google AI Platform"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "tensorflow==2.1.0\n",
    "tensorflow-datasets==2.1.0\n",
    "kubeflow-fairing==0.7.1\n",
    "google-cloud-storage==1.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Class"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tfds.disable_progress_bar()\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from google.cloud import storage\n",
    "import re\n",
    "import logging\n",
    "\n",
    "_GCS_PREFIX = \"gs://\"\n",
    "\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "    level=logging.INFO)\n",
    "print('Tensorflow-version: {0}'.format(tf.__version__))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Storage(object):\n",
    "    @staticmethod\n",
    "    def upload(uri: str, out_dir: str = None) -> str:\n",
    "        logging.info(\"Copying contents from %s to %s\", uri, out_dir)\n",
    "\n",
    "        if out_dir.startswith(_GCS_PREFIX):\n",
    "            Storage._upload_gcs(uri, out_dir)\n",
    "        else:\n",
    "            raise Exception(\"Cannot recognize storage type for \" + uri +\n",
    "                            \"\\n'%s' are the current available storage type.\" %\n",
    "                            (_GCS_PREFIX))\n",
    "\n",
    "        logging.info(\"Successfully copied %s to %s\", uri, out_dir)\n",
    "        return out_dir\n",
    "    \n",
    "    @staticmethod\n",
    "    def _upload_gcs(uri, out_dir: str):\n",
    "        try:\n",
    "            storage_client = storage.Client()\n",
    "        except exceptions.DefaultCredentialsError:\n",
    "            storage_client = storage.Client.create_anonymous_client()\n",
    "        \n",
    "        bucket_args = out_dir.replace(_GCS_PREFIX, \"\", 1).split(\"/\", 1)\n",
    "        bucket_name = bucket_args[0]\n",
    "        gcs_path = bucket_args[1] if len(bucket_args) > 1 else \"\"\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        Storage.upload_local_directory_to_gcs(uri,bucket, gcs_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def upload_local_directory_to_gcs(local_path, bucket, gcs_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            if not os.path.isfile(local_file):\n",
    "                Storage.upload_local_directory_to_gcs(local_file, bucket, gcs_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(gcs_path, local_file[1 + len(local_path):])\n",
    "                blob = bucket.blob(remote_path)\n",
    "                blob.upload_from_filename(local_file)\n",
    "\n",
    "\n",
    "class TensorflowModel(object):\n",
    "    \n",
    "    def __init__(self, bucket=None,export=True):\n",
    "        self.model = None\n",
    "        self.export_dir = \"{0}/export/001\".format(bucket)\n",
    "        self.export = export\n",
    "        \n",
    "    # prepare data\n",
    "    def prepare_data(self, batch_size=64, shuffle_size=1000):\n",
    "\n",
    "        def scale(image, label):\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            image /= 255\n",
    "            return image, label\n",
    "\n",
    "        # Split the training set into 80% and 20% for training and validation\n",
    "        train_validation_split = tfds.Split.TRAIN.subsplit([8, 2])\n",
    "        ((train_data, validation_data), test_data),info = tfds.load(name=\"fashion_mnist:1.0.0\", \n",
    "                                                             split=(train_validation_split, tfds.Split.TEST),\n",
    "                                                             as_supervised=True, with_info=True)\n",
    "\n",
    "\n",
    "        print(\"Training data count : \", int(info.splits['train'].num_examples * 0.8))\n",
    "        print(\"Validation data count : \", int(info.splits['train'].num_examples * 0.2))\n",
    "        print(\"Test data count : \", int(info.splits['test'].num_examples))\n",
    "\n",
    "\n",
    "        # create dataset to be used for training process\n",
    "        train_dataset = train_data.map(scale).shuffle(shuffle_size).batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        val_dataset = validation_data.map(scale).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        test_dataset = test_data.map(scale).batch(batch_size)\n",
    "\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    # build model\n",
    "    def build_model(self, learning_rate=0.001):\n",
    "        # define model architecture\n",
    "        model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28, 28, 1), name='x'),\n",
    "          tf.keras.layers.MaxPooling2D(),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(64, activation='relu'),\n",
    "          tf.keras.layers.Dense(10, activation='softmax')\n",
    "          ])\n",
    "        # compile model with loss, optimizer and accuracy \n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    # callback\n",
    "    def get_callbacks(self):\n",
    "        # callbacks \n",
    "        # folder to store current training logs\n",
    "        logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        class customLog(tf.keras.callbacks.Callback):\n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                logging.info('epoch: {}'.format(epoch + 1))\n",
    "                logging.info('loss={}'.format(logs['loss']))\n",
    "                logging.info('accuracy={}'.format(logs['accuracy']))\n",
    "                logging.info('val_accuracy={}'.format(logs['val_accuracy']))\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.TensorBoard(logdir),\n",
    "            customLog()\n",
    "        ]\n",
    "        return callbacks\n",
    "    \n",
    "    # train model\n",
    "    def train(self):\n",
    "        # Data extraction and processing\n",
    "        # set variables\n",
    "        BUFFER_SIZE = 10000\n",
    "        BATCH_SIZE = 64\n",
    "\n",
    "        train_dataset, val_dataset, test_dataset = self.prepare_data(batch_size=BATCH_SIZE, shuffle_size=BUFFER_SIZE)\n",
    "\n",
    "        # Build Model\n",
    "        TF_LEARNING_RATE = 0.001\n",
    "        model = self.build_model(learning_rate=TF_LEARNING_RATE)\n",
    "        model.summary()\n",
    "\n",
    "        # train model\n",
    "        TF_EPOCHS=10\n",
    "        TF_STEPS_PER_EPOCHS=3\n",
    "        #TF_STEPS_PER_EPOCHS = int(np.ceil(48000 / float(BATCH_SIZE)))  # number of training samples / batch size\n",
    "\n",
    "        model.fit(train_dataset, \n",
    "                  epochs=TF_EPOCHS,\n",
    "                  steps_per_epoch=3,\n",
    "                  validation_data=val_dataset,\n",
    "                  callbacks=self.get_callbacks())\n",
    "        \n",
    "        # evaluate model\n",
    "        result = model.evaluate(test_dataset, steps=1)\n",
    "        loss = result[0]\n",
    "        accuracy = result[1]\n",
    "        print(\"loss : {0} accuracy : {1}\".format(loss, accuracy))\n",
    "        \n",
    "        \n",
    "        # save and export model\n",
    "        tf.saved_model.save(model, \"/tmp/export\")\n",
    "        if self.export:\n",
    "            Storage.upload(\"/tmp/export\", self.export_dir)\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        # use this method to setup prediction\n",
    "        pass \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train locally"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorflowModel(export=False).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Kubernetes Cluster"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow import fairing\n",
    "import importlib\n",
    "from kubeflow.fairing import TrainJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "GCS_BUCKET_ID = \"{}-fashion-mnist-fairing-k8s-backend\".format(GCP_PROJECT)\n",
    "GCS_BUCKET = \"gs://{}\".format(GCS_BUCKET_ID)\n",
    "!gsutil rm -r {GCS_BUCKET}\n",
    "!gsutil mb {GCS_BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "DOCKER_REGISTRY = 'gcr.io/{}/fairing-job-mnist'.format(GCP_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairing supports multiple backend such as \n",
    "\n",
    "- KubeflowGKEBackend (Google: GKE Cluster)\n",
    "- KubeflowAzureBackend (Azure)\n",
    "- KubeflowAWSBackend (AWS)\n",
    "- GKEManagedBackend (Google AI Platform)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set fairing backend\n",
    "FAIRING_BACKEND = 'KubeflowGKEBackend'\n",
    "\n",
    "BuildContext = None\n",
    "BackendClass = getattr(importlib.import_module('kubeflow.fairing.backends'), FAIRING_BACKEND)\n",
    "train_job = TrainJob(lambda : TensorflowModel(bucket=GCS_BUCKET,export=True), input_files=[\"requirements.txt\"],\n",
    "                     docker_registry=DOCKER_REGISTRY,\n",
    "                     backend=BackendClass(build_context_source=BuildContext))\n",
    "\n",
    "train_job.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}